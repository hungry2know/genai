{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNtacXHuEtiQ1WtnW1H3BQu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hungry2know/genai/blob/main/RAG_With_Knowledge_graph(Neo4j).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing environment variables for google cloud and Neo4J cloud instance\n"
      ],
      "metadata": {
        "id": "R5-kIdJ0owqq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvKh2btSjc_t"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from google.colab import userdata\n",
        "os.environ[\"GOOGLE_API_KEY\"]=userdata.get('GOOGLE_API_KEY')\n",
        "os.environ[\"GOOGLE_CLOUD_LOCATION\"]=userdata.get('GOOGLE_CLOUD_LOCATION')\n",
        "os.environ[\"GOOGLE_CLOUD_PROJECT\"]=userdata.get('GOOGLE_CLOUD_PROJECT')\n",
        "os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"]=userdata.get('GOOGLE_GENAI_USE_VERTEXAI')\n",
        "os.environ[\"NEO4J_PASSWORD\"]=userdata.get('NEO4J_PASSWORD')\n",
        "os.environ[\"NEO4J_URI\"]=userdata.get('NEO4J_URI')\n",
        "os.environ[\"NEO4J_USERNAME\"]=userdata.get('NEO4J_USERNAME')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing dependencies"
      ],
      "metadata": {
        "id": "isKBLgtarElV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade langchain langchain-community langchain-google-genai langchain-google-vertexai langchain-experimental neo4j wikipedia tiktoken yfiles_jupyter_graphs"
      ],
      "metadata": {
        "collapsed": true,
        "id": "C9NPnImerHec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Google Authentication"
      ],
      "metadata": {
        "id": "JO6gEx-EcCSS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "!gcloud config set project {os.environ[\"GOOGLE_CLOUD_PROJECT\"]}\n",
        "!gcloud auth application-default set-quota-project {os.environ[\"GOOGLE_CLOUD_PROJECT\"]}"
      ],
      "metadata": {
        "id": "fQVvRK8ycB-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading ANZ (bank) article from wikipedia in Neo4j graph"
      ],
      "metadata": {
        "id": "ieYed-V-tgWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import WikipediaLoader\n",
        "raw_documents = WikipediaLoader(query=\"ANZ (bank)\").load()\n",
        "\n",
        "from langchain.text_splitter import TokenTextSplitter\n",
        "text_splitter = TokenTextSplitter(chunk_size=512, chunk_overlap=24)\n",
        "documents = text_splitter.split_documents(raw_documents)\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "llm = ChatGoogleGenerativeAI(temperature=0, model=\"gemini-2.5-flash\")\n",
        "\n",
        "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
        "llm_transformer = LLMGraphTransformer(llm=llm)\n",
        "\n",
        "graph_documents = llm_transformer.convert_to_graph_documents(documents)\n",
        "\n",
        "from langchain_community.graphs import Neo4jGraph\n",
        "graph = Neo4jGraph()\n",
        "\n",
        "graph.query(\"CREATE FULLTEXT INDEX entity IF NOT EXISTS FOR (e:__Entity__) ON EACH [e.id]\")\n",
        "\n",
        "graph.add_graph_documents(\n",
        "    graph_documents,\n",
        "    baseEntityLabel=True,\n",
        "    include_source=True\n",
        ")"
      ],
      "metadata": {
        "id": "qVfdSFbUsIQ2",
        "outputId": "cf4696db-0892-4c31-c023-76e06396ee58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval"
      ],
      "metadata": {
        "id": "QxvDt7aqvHbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "!gcloud config set project {os.environ[\"GOOGLE_CLOUD_PROJECT\"]}\n",
        "!gcloud auth application-default set-quota-project {os.environ[\"GOOGLE_CLOUD_PROJECT\"]}\n",
        "\n",
        "from langchain_community.vectorstores import Neo4jVector\n",
        "\n",
        "from langchain_google_vertexai import VertexAIEmbeddings\n",
        "\n",
        "vector_index = Neo4jVector.from_existing_graph(\n",
        "    VertexAIEmbeddings(model_name=\"text-embedding-005\"),\n",
        "    search_type=\"hybrid\",\n",
        "    node_label=\"Document\",\n",
        "    text_node_properties=[\"text\"],\n",
        "    embedding_node_property=\"embedding\"\n",
        ")"
      ],
      "metadata": {
        "id": "V205QYZKvsNk",
        "outputId": "1cff2094-377a-4687-c032-9d9388dd253f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n",
            "\n",
            "Credentials saved to file: [/content/.config/application_default_credentials.json]\n",
            "\n",
            "These credentials will be used by any library that requests Application Default Credentials (ADC).\n",
            "\n",
            "Quota project \"learn-langchain-466102\" was added to ADC which can be used by Google client libraries for billing and quota. Note that some services may still bill the project owning the resource.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/vertexai/_model_garden/_model_garden_models.py:278: UserWarning: This feature is deprecated as of June 24, 2025 and will be removed on June 24, 2026. For details, see https://cloud.google.com/vertex-ai/generative-ai/docs/deprecations/genai-vertexai-sdk.\n",
            "  warning_logs.show_deprecation_warning()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from typing import Tuple, List, Optional\n",
        "\n",
        "# Extract entities from text\n",
        "class Entities(BaseModel):\n",
        "    \"\"\"Identifying information about entities.\"\"\"\n",
        "\n",
        "    names: List[str] = Field(\n",
        "        ...,\n",
        "        description=\"All the person, organization, or business entities that \"\n",
        "        \"appear in the text\",\n",
        "    )\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.prompts.prompt import PromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are extracting organization and person entities from the text.\",\n",
        "        ),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"Use the given format to extract information from the following \"\n",
        "            \"input: {question}\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "entity_chain = prompt | llm.with_structured_output(Entities)\n",
        "\n",
        "entity_chain.invoke({\"question\": \"Who is latest CEO of ANZ bank?\"}).names"
      ],
      "metadata": {
        "id": "J_sbm6qP42q5",
        "outputId": "fb9b3b36-48c0-455f-9e1b-1abf64010dc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ANZ bank']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores.neo4j_vector import remove_lucene_chars\n",
        "\n",
        "def generate_full_text_query(input: str) -> str:\n",
        "    full_text_query = \"\"\n",
        "    words = [el for el in remove_lucene_chars(input).split() if el]\n",
        "    for word in words[:-1]:\n",
        "        full_text_query += f\" {word}~2 AND\"\n",
        "    full_text_query += f\" {words[-1]}~2\"\n",
        "    return full_text_query.strip()\n",
        "\n",
        "\n",
        "def structured_retriever(question: str) -> str:\n",
        "    result = \"\"\n",
        "    entities = entity_chain.invoke({\"question\": question})\n",
        "    for entity in entities.names:\n",
        "        response = graph.query(\n",
        "            \"\"\"CALL db.index.fulltext.queryNodes('entity', $query, {limit:2})\n",
        "            YIELD node,score\n",
        "            CALL {\n",
        "              WITH node\n",
        "              MATCH (node)-[r:!MENTIONS]->(neighbor)\n",
        "              RETURN node.id + ' - ' + type(r) + ' -> ' + neighbor.id AS output\n",
        "              UNION ALL\n",
        "              WITH node\n",
        "              MATCH (node)<-[r:!MENTIONS]-(neighbor)\n",
        "              RETURN neighbor.id + ' - ' + type(r) + ' -> ' +  node.id AS output\n",
        "            }\n",
        "            RETURN output LIMIT 50\n",
        "            \"\"\",\n",
        "            {\"query\": generate_full_text_query(entity)},\n",
        "        )\n",
        "        result += \"\\n\".join([el['output'] for el in response])\n",
        "    return result"
      ],
      "metadata": {
        "id": "Arc2azv14pWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "from langchain_core.runnables import (\n",
        "    RunnableBranch,\n",
        "    RunnableLambda,\n",
        "    RunnableParallel,\n",
        "    RunnablePassthrough,\n",
        ")\n",
        "\n",
        "def retriever(question: str):\n",
        "    print(f\"Search query: {question}\")\n",
        "    structured_data = structured_retriever(question)\n",
        "    print(f\"structured_data : {structured_data}\")\n",
        "    unstructured_data = [el.page_content for el in vector_index.similarity_search(question)]\n",
        "    final_data = f\"\"\"Structured data:\n",
        "{structured_data}\n",
        "Unstructured data:\n",
        "{\"#Document \". join(unstructured_data)}\n",
        "    \"\"\"\n",
        "    return final_data\n",
        "\n",
        "\n",
        "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question,\n",
        "in its original language.\n",
        "Chat History:\n",
        "{chat_history}\n",
        "Follow Up Input: {question}\n",
        "Standalone question:\"\"\"\n",
        "\n",
        "\n",
        "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
        "\n",
        "\n",
        "def _format_chat_history(chat_history: List[Tuple[str, str]]) -> List:\n",
        "    buffer = []\n",
        "    for human, ai in chat_history:\n",
        "        buffer.append(HumanMessage(content=human))\n",
        "        buffer.append(AIMessage(content=ai))\n",
        "    return buffer\n",
        "\n",
        "\n",
        "\n",
        "_search_query = RunnableBranch(\n",
        "    # If input includes chat_history, we condense it with the follow-up question\n",
        "    (\n",
        "        RunnableLambda(lambda x: bool(x.get(\"chat_history\"))).with_config(\n",
        "            run_name=\"HasChatHistoryCheck\"\n",
        "        ),  # Condense follow-up question and chat into a standalone_question\n",
        "        RunnablePassthrough.assign(\n",
        "            chat_history=lambda x: _format_chat_history(x[\"chat_history\"])\n",
        "        )\n",
        "        | CONDENSE_QUESTION_PROMPT\n",
        "        | ChatGoogleGenerativeAI(temperature=0, model=\"gemini-2.5-flash\")\n",
        "        | StrOutputParser(),\n",
        "    ),\n",
        "    # Else, we have no chat history, so just pass through the question\n",
        "    RunnableLambda(lambda x : x[\"question\"]),\n",
        ")\n",
        "\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "Use natural language and be concise.\n",
        "Answer:\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "\n",
        "chain = (\n",
        "    RunnableParallel(\n",
        "        {\n",
        "            \"context\": _search_query | retriever,\n",
        "            \"question\": RunnablePassthrough(),\n",
        "        }\n",
        "    )\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "chain.invoke({\"question\": \"Who is latest CEO of ANZ bank?\"})"
      ],
      "metadata": {
        "id": "n9adsSnu5YFZ",
        "outputId": "cb2634ec-f722-4e97-b59d-3b9c4dd36d11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 769
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search query: Who is latest CEO of ANZ bank?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:neo4j.notifications:Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: CALL subquery without a variable scope clause is now deprecated. Use CALL (node, node) { ... }} {position: line: 3, column: 13, offset: 104} for query: \"CALL db.index.fulltext.queryNodes('entity', $query, {limit:2})\\n            YIELD node,score\\n            CALL {\\n              WITH node\\n              MATCH (node)-[r:!MENTIONS]->(neighbor)\\n              RETURN node.id + ' - ' + type(r) + ' -> ' + neighbor.id AS output\\n              UNION ALL\\n              WITH node\\n              MATCH (node)<-[r:!MENTIONS]-(neighbor)\\n              RETURN neighbor.id + ' - ' + type(r) + ' -> ' +  node.id AS output\\n            }\\n            RETURN output LIMIT 50\\n            \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "structured_data : Launceston - LOCATED_IN -> Van Diemens Land\n",
            "Anz Bank - HEADQUARTERED_IN -> Melbourne\n",
            "Anz Bank - LOCATED_IN -> Australia\n",
            "Anz Bank - IS_A_TYPE_OF -> Major Bank\n",
            "Anz Bank - OPERATES_IN -> Australia\n",
            "Anz Bank - OPERATES_IN -> New Zealand\n",
            "Anz Bank - FORMED_BY_MERGER_OF -> English, Scottish & Australian Bank\n",
            "Anz Bank - FORMED_BY_MERGER_OF -> Australia And New Zealand Bank\n",
            "Anz Bank - IS_MEMBER_OF -> Big Four Australian Banks\n",
            "Anz Bank - LARGEST_BANK_IN -> New Zealand\n",
            "Anz Bank - OPERATED_AS -> Anz National Bank\n",
            "Anz Bank - OPERATED_AS -> Anz Bank New Zealand\n",
            "Anz Bank - OPERATED_BRAND -> National Bank Of New Zealand\n",
            "Anz Bank - ACQUIRED -> National Bank Of New Zealand\n",
            "Anz Bank - ACQUIRED -> Postbank\n",
            "Anz Bank - ACQUIRED -> Ing New Zealand Limited\n",
            "Anz Bank - ESTABLISHED_FACILITY -> Data Processing Centre\n",
            "Anz Bank - BEGAN_OPERATIONS_IN -> Honiara\n",
            "Anz Bank - OPENED_OFFICE_IN -> New York\n",
            "Anz Bank - ESTABLISHED_OFFICE_IN -> Tokyo\n",
            "Anz Bank - MERGED_WITH -> English, Scottish & Australian Bank\n",
            "Anz Bank - MERGED_WITH -> English, Scottish And Australian Bank\n",
            "Anz Bank - HAS_BRANCH_IN -> Auckland\n",
            "Anz Bank - HAS_BRANCH_IN -> Dunedin\n",
            "Anz Bank - HAS_BRANCH_IN -> Christchurch\n",
            "Anz Bank - FORMED_FROM -> Bank Of Australasia\n",
            "Anz Bank - FORMED_FROM -> Uba\n",
            "Anz Bank - JOINED -> Databank Systems Limited\n",
            "Anz Bank - HAS_HEADQUARTERS_IN -> Melbourne\n",
            "Anz Bank - HAS_BRANCHES_IN -> New Zealand\n",
            "Anz Bank - HAS_HEAD_OFFICE_IN -> Wellington\n",
            "Anz Bank - MOVED_JOBS_TO -> Bangalore\n",
            "Anz Bank - MEMBER_OF_GROUP -> Big Four Australian Banks\n",
            "Anz Bank - HAS_LEGAL_ENTITY -> Anz National Bank\n",
            "Anz Bank - HAS_NAMING_RIGHTS_FOR -> Anz Bank Centre\n",
            "Ing - FORMED_JOINT_VENTURE_WITH -> Anz Bank\n",
            "Mike Smith - IS_CEO_OF -> Anz Bank\n",
            "National Bank Of New Zealand - BRAND_RETIRED_BY -> Anz Bank\n",
            "Anz Bank Centre - HAS_MAJOR_TENANT -> Anz Bank\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:neo4j.notifications:Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: CALL subquery without a variable scope clause is now deprecated. Use CALL () { ... }} {position: line: 1, column: 1, offset: 0} for query: \"CALL { CALL db.index.vector.queryNodes($index, $k, $embedding) YIELD node, score WITH collect({node:node, score:score}) AS nodes, max(score) AS max UNWIND nodes AS n RETURN n.node AS node, (n.score / max) AS score UNION CALL db.index.fulltext.queryNodes($keyword_index, $query, {limit: $k}) YIELD node, score WITH collect({node:node, score:score}) AS nodes, max(score) AS max UNWIND nodes AS n RETURN n.node AS node, (n.score / max) AS score } WITH node, max(score) AS score ORDER BY score DESC LIMIT $k RETURN reduce(str='', k IN ['text'] | str + '\\\\n' + k + ': ' + coalesce(node[k], '')) AS text, node {.*, `embedding`: Null, id: Null, `text`: Null} AS metadata, score\"\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Mike Smith.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    }
  ]
}