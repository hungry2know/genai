{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNtacXHuEtiQ1WtnW1H3BQu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hungry2know/genai/blob/main/RAG_With_Knowledge_graph(Neo4j).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing environment variables for google cloud and Neo4J cloud instance\n"
      ],
      "metadata": {
        "id": "R5-kIdJ0owqq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvKh2btSjc_t"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from google.colab import userdata\n",
        "os.environ[\"GOOGLE_API_KEY\"]=userdata.get('GOOGLE_API_KEY')\n",
        "os.environ[\"GOOGLE_CLOUD_LOCATION\"]=userdata.get('GOOGLE_CLOUD_LOCATION')\n",
        "os.environ[\"GOOGLE_CLOUD_PROJECT\"]=userdata.get('GOOGLE_CLOUD_PROJECT')\n",
        "os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"]=userdata.get('GOOGLE_GENAI_USE_VERTEXAI')\n",
        "os.environ[\"NEO4J_PASSWORD\"]=userdata.get('NEO4J_PASSWORD')\n",
        "os.environ[\"NEO4J_URI\"]=userdata.get('NEO4J_URI')\n",
        "os.environ[\"NEO4J_USERNAME\"]=userdata.get('NEO4J_USERNAME')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing dependencies"
      ],
      "metadata": {
        "id": "isKBLgtarElV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade langchain langchain-community langchain-google-genai langchain-google-vertexai langchain-experimental neo4j wikipedia tiktoken yfiles_jupyter_graphs"
      ],
      "metadata": {
        "collapsed": true,
        "id": "C9NPnImerHec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Google Authentication"
      ],
      "metadata": {
        "id": "JO6gEx-EcCSS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "!gcloud config set project {os.environ[\"GOOGLE_CLOUD_PROJECT\"]}\n",
        "!gcloud auth application-default set-quota-project {os.environ[\"GOOGLE_CLOUD_PROJECT\"]}"
      ],
      "metadata": {
        "id": "fQVvRK8ycB-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading ANZ (bank) article from wikipedia in Neo4j graph"
      ],
      "metadata": {
        "id": "ieYed-V-tgWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import WikipediaLoader\n",
        "raw_documents = WikipediaLoader(query=\"ANZ (bank)\").load()\n",
        "\n",
        "from langchain.text_splitter import TokenTextSplitter\n",
        "text_splitter = TokenTextSplitter(chunk_size=512, chunk_overlap=24)\n",
        "documents = text_splitter.split_documents(raw_documents)\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "llm = ChatGoogleGenerativeAI(temperature=0, model=\"gemini-2.5-flash\")\n",
        "\n",
        "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
        "llm_transformer = LLMGraphTransformer(llm=llm)\n",
        "\n",
        "graph_documents = llm_transformer.convert_to_graph_documents(documents)\n",
        "\n",
        "from langchain_community.graphs import Neo4jGraph\n",
        "graph = Neo4jGraph()\n",
        "\n",
        "graph.query(\"CREATE FULLTEXT INDEX entity IF NOT EXISTS FOR (e:__Entity__) ON EACH [e.id]\")\n",
        "\n",
        "graph.add_graph_documents(\n",
        "    graph_documents,\n",
        "    baseEntityLabel=True,\n",
        "    include_source=True\n",
        ")"
      ],
      "metadata": {
        "id": "qVfdSFbUsIQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval"
      ],
      "metadata": {
        "id": "QxvDt7aqvHbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "!gcloud config set project {os.environ[\"GOOGLE_CLOUD_PROJECT\"]}\n",
        "!gcloud auth application-default set-quota-project {os.environ[\"GOOGLE_CLOUD_PROJECT\"]}\n",
        "\n",
        "from langchain_community.vectorstores import Neo4jVector\n",
        "\n",
        "from langchain_google_vertexai import VertexAIEmbeddings\n",
        "\n",
        "vector_index = Neo4jVector.from_existing_graph(\n",
        "    VertexAIEmbeddings(model_name=\"text-embedding-005\"),\n",
        "    search_type=\"hybrid\",\n",
        "    node_label=\"Document\",\n",
        "    text_node_properties=[\"text\"],\n",
        "    embedding_node_property=\"embedding\"\n",
        ")"
      ],
      "metadata": {
        "id": "V205QYZKvsNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from typing import Tuple, List, Optional\n",
        "\n",
        "# Extract entities from text\n",
        "class Entities(BaseModel):\n",
        "    \"\"\"Identifying information about entities.\"\"\"\n",
        "\n",
        "    names: List[str] = Field(\n",
        "        ...,\n",
        "        description=\"All the person, organization, or business entities that \"\n",
        "        \"appear in the text\",\n",
        "    )\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.prompts.prompt import PromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are extracting organization and person entities from the text.\",\n",
        "        ),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"Use the given format to extract information from the following \"\n",
        "            \"input: {question}\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "entity_chain = prompt | llm.with_structured_output(Entities)\n",
        "\n",
        "entity_chain.invoke({\"question\": \"Who is latest CEO of ANZ bank?\"}).names"
      ],
      "metadata": {
        "id": "J_sbm6qP42q5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores.neo4j_vector import remove_lucene_chars\n",
        "\n",
        "def generate_full_text_query(input: str) -> str:\n",
        "    full_text_query = \"\"\n",
        "    words = [el for el in remove_lucene_chars(input).split() if el]\n",
        "    for word in words[:-1]:\n",
        "        full_text_query += f\" {word}~2 AND\"\n",
        "    full_text_query += f\" {words[-1]}~2\"\n",
        "    return full_text_query.strip()\n",
        "\n",
        "\n",
        "def structured_retriever(question: str) -> str:\n",
        "    result = \"\"\n",
        "    entities = entity_chain.invoke({\"question\": question})\n",
        "    for entity in entities.names:\n",
        "        response = graph.query(\n",
        "            \"\"\"CALL db.index.fulltext.queryNodes('entity', $query, {limit:2})\n",
        "            YIELD node,score\n",
        "            CALL {\n",
        "              WITH node\n",
        "              MATCH (node)-[r:!MENTIONS]->(neighbor)\n",
        "              RETURN node.id + ' - ' + type(r) + ' -> ' + neighbor.id AS output\n",
        "              UNION ALL\n",
        "              WITH node\n",
        "              MATCH (node)<-[r:!MENTIONS]-(neighbor)\n",
        "              RETURN neighbor.id + ' - ' + type(r) + ' -> ' +  node.id AS output\n",
        "            }\n",
        "            RETURN output LIMIT 50\n",
        "            \"\"\",\n",
        "            {\"query\": generate_full_text_query(entity)},\n",
        "        )\n",
        "        result += \"\\n\".join([el['output'] for el in response])\n",
        "    return result"
      ],
      "metadata": {
        "id": "Arc2azv14pWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "from langchain_core.runnables import (\n",
        "    RunnableBranch,\n",
        "    RunnableLambda,\n",
        "    RunnableParallel,\n",
        "    RunnablePassthrough,\n",
        ")\n",
        "\n",
        "def retriever(question: str):\n",
        "    print(f\"Search query: {question}\")\n",
        "    structured_data = structured_retriever(question)\n",
        "    print(f\"structured_data : {structured_data}\")\n",
        "    unstructured_data = [el.page_content for el in vector_index.similarity_search(question)]\n",
        "    final_data = f\"\"\"Structured data:\n",
        "{structured_data}\n",
        "Unstructured data:\n",
        "{\"#Document \". join(unstructured_data)}\n",
        "    \"\"\"\n",
        "    return final_data\n",
        "\n",
        "\n",
        "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question,\n",
        "in its original language.\n",
        "Chat History:\n",
        "{chat_history}\n",
        "Follow Up Input: {question}\n",
        "Standalone question:\"\"\"\n",
        "\n",
        "\n",
        "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
        "\n",
        "\n",
        "def _format_chat_history(chat_history: List[Tuple[str, str]]) -> List:\n",
        "    buffer = []\n",
        "    for human, ai in chat_history:\n",
        "        buffer.append(HumanMessage(content=human))\n",
        "        buffer.append(AIMessage(content=ai))\n",
        "    return buffer\n",
        "\n",
        "\n",
        "\n",
        "_search_query = RunnableBranch(\n",
        "    # If input includes chat_history, we condense it with the follow-up question\n",
        "    (\n",
        "        RunnableLambda(lambda x: bool(x.get(\"chat_history\"))).with_config(\n",
        "            run_name=\"HasChatHistoryCheck\"\n",
        "        ),  # Condense follow-up question and chat into a standalone_question\n",
        "        RunnablePassthrough.assign(\n",
        "            chat_history=lambda x: _format_chat_history(x[\"chat_history\"])\n",
        "        )\n",
        "        | CONDENSE_QUESTION_PROMPT\n",
        "        | ChatGoogleGenerativeAI(temperature=0, model=\"gemini-2.5-flash\")\n",
        "        | StrOutputParser(),\n",
        "    ),\n",
        "    # Else, we have no chat history, so just pass through the question\n",
        "    RunnableLambda(lambda x : x[\"question\"]),\n",
        ")\n",
        "\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "Use natural language and be concise.\n",
        "Answer:\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "\n",
        "chain = (\n",
        "    RunnableParallel(\n",
        "        {\n",
        "            \"context\": _search_query | retriever,\n",
        "            \"question\": RunnablePassthrough(),\n",
        "        }\n",
        "    )\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "chain.invoke({\"question\": \"Who is latest CEO of ANZ bank?\"})"
      ],
      "metadata": {
        "id": "n9adsSnu5YFZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}